{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links to shared data MovieLens\n",
    "# source on kaggle: https://www.kaggle.com/code/quangnhatbui/movie-recommender/data\n",
    "RATINGS_SMALL_URL = 'https://drive.google.com/file/d/1BlZfCLLs5A13tbNSJZ1GPkHLWQOnPlE4/view?usp=share_link'\n",
    "MOVIES_METADATA_URL = 'https://drive.google.com/file/d/19g6-apYbZb5D-wRj4L7aYKhxS-fDM4Fb/view?usp=share_link'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to make it available to download w/o SSL verification\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from itertools import permutations\n",
    "\n",
    "import torch\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Helper functions to avoid copy paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_gdrive(url):\n",
    "    \"\"\"\n",
    "    gets csv data from a given url (taken from file -> share -> copy link)\n",
    "    :url: example https://drive.google.com/file/d/1BlZfCLLs5A13tbNSJZ1GPkHLWQOnPlE4/view?usp=share_link\n",
    "    \"\"\"\n",
    "    file_id = url.split('/')[-2]\n",
    "    file_path = 'https://drive.google.com/uc?export=download&id=' + file_id\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Main"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. RankNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankNet(torch.nn.Module):\n",
    "    def __init__(self, input_features_len, hidden_dim = 10):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_features_len, self.hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.hidden_dim, 1),\n",
    "        )\n",
    "        \n",
    "        self.out_activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_1, input_2):\n",
    "        logits_1 = self.predict(input_1)\n",
    "        logits_2 = self.predict(input_2)\n",
    "        \n",
    "        logits_diff = logits_1 - logits_2\n",
    "        out = self.out_activation(logits_diff)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def predict(self, inp):\n",
    "        logits = self.model(inp)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RankNet(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=10, out_features=1, bias=True)\n",
       "  )\n",
       "  (out_activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RankNet(input_features_len = 8)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7056, 0.4304, 0.7819, 0.9265, 0.5223, 0.5009, 0.1768, 0.3562],\n",
       "        [0.9407, 0.6271, 0.3754, 0.8852, 0.2320, 0.5111, 0.8795, 0.7290],\n",
       "        [0.0043, 0.4087, 0.9889, 0.2314, 0.0170, 0.7701, 0.5280, 0.8738],\n",
       "        [0.4480, 0.0341, 0.1732, 0.2917, 0.0119, 0.4544, 0.3268, 0.5849]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1, input_2 = torch.rand(4, 8), torch.rand(4, 8)\n",
    "input_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5334],\n",
       "        [0.4944],\n",
       "        [0.4824],\n",
       "        [0.4794]], grad_fn=<SortBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = torch.sort(model(input_1, input_2), descending = True, dim = 0)\n",
    "preds[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. ListNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ozark', 'Avatar', 'The Godfather')\n",
      "('Ozark', 'The Godfather', 'Avatar')\n",
      "('Avatar', 'Ozark', 'The Godfather')\n",
      "('Avatar', 'The Godfather', 'Ozark')\n",
      "('The Godfather', 'Ozark', 'Avatar')\n",
      "('The Godfather', 'Avatar', 'Ozark')\n"
     ]
    }
   ],
   "source": [
    "movies_to_rank = {'The Godfather', 'Avatar', 'Ozark'}\n",
    "permutations_list = list(permutations(movies_to_rank))\n",
    "\n",
    "for i in permutations_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The Godfather', 'Avatar', 'Ozark')\n"
     ]
    }
   ],
   "source": [
    "pi = random.choice(permutations_list)\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ozark': 0.4967141530112327, 'Avatar': -0.13826430117118466, 'The Godfather': 0.6476885381006925}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "scores_dict = {x: np.random.randn(1)[0] for x in movies_to_rank}  \n",
    "print(scores_dict)\n",
    "\n",
    "# unpack pi and assign movies to scores\n",
    "score_movie_pos_1, score_movie_pos_2, score_movie_pos_3 = scores_dict[pi[0]], scores_dict[pi[1]], scores_dict[pi[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First term is: 0.4318619033836114\n",
      "Second term is: 0.3463825470936087\n",
      "Third term is: 1.0\n"
     ]
    }
   ],
   "source": [
    "first_term = np.exp(score_movie_pos_1) / (np.exp(score_movie_pos_1) + np.exp(score_movie_pos_2)\\\n",
    "                                         + np.exp(score_movie_pos_3))\n",
    "\n",
    "second_term = np.exp(score_movie_pos_2) / (np.exp(score_movie_pos_2) + np.exp(score_movie_pos_3))\n",
    "\n",
    "third_term = np.exp(score_movie_pos_3) / np.exp(score_movie_pos_3)\n",
    "\n",
    "print(f'First term is: {first_term}')\n",
    "print(f'Second term is: {second_term}')\n",
    "print(f'Third term is: {third_term}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P_{s}(<The Godfather, Avatar, Ozark>) = \\prod^3_{j = 1} \\frac {\\phi(s_{\\pi(j)})} {\\sum^3_{k = j} \\phi(s_{\\pi(k)})}$ which is equal to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation probability is: 0.14958942608670928\n"
     ]
    }
   ],
   "source": [
    "permutation_proba = first_term * second_term * third_term\n",
    "\n",
    "print(f'Permutation probability is: {permutation_proba}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. CatBoost Ranker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`interactions` dataset shows list of movies that users watched, along with given ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactions data\n",
    "interactions = read_csv_from_gdrive(RATINGS_SMALL_URL)\n",
    "interactions.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`movies_metadata` dataset shows the list of movies existing on OKKO platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information about films etc\n",
    "movies_metadata = read_csv_from_gdrive(MOVIES_METADATA_URL)\n",
    "movies_metadata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_metadata['id'] = movies_metadata['id'].astype(str)\n",
    "interactions['movieId'] = interactions['movieId'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_metadata.rename(columns = {'id': 'movieId'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave only those films that intersect with each other\n",
    "interactions_filtered = interactions.loc[interactions['movieId'].isin(movies_metadata['id'])]\n",
    "print(interactions.shape, interactions_filtered.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = .25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp to date\n",
    "interactions_filtered['dttm'] = interactions_filtered['timestamp']\\\n",
    "                                .apply(lambda x: pd.to_datetime(dt.datetime.fromtimestamp(x).strftime('%Y-%m-%d')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate some basic features, but keep in mind that our data of historical ratings depends on time.\n",
    "We need to avoid data leak -- use future values in past data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_FEATURES_LIST = ['revenue', 'budget', 'runtime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate avg ratings by users and items daily\n",
    "daily_users_feature = interactions_filtered.groupby(['userId', 'dttm']).agg({'rating': 'mean',\n",
    "                                              'movieId': 'count'})\\\n",
    "                                  .reset_index().sort_values(['userId', 'dttm'])\\\n",
    "                                  .rename(columns = {'rating': 'user_mean_rating',\n",
    "                                                     'movieId': 'user_watch_count'})\n",
    "\n",
    "\n",
    "daily_users_feature['dttm'] = daily_users_feature['dttm'].apply(lambda x: x + dt.timedelta(days = 1))\n",
    "daily_users_feature.loc[daily_users_feature['userId'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_users_feature.loc[daily_users_feature['userId'] == 671]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_total_cnt = daily_users_feature.set_index('dttm').groupby(['userId'])['user_watch_count']\\\n",
    "                        .rolling(window = 3, min_periods = 1).sum()\\\n",
    "                        .reset_index()[['userId', 'dttm', 'user_watch_count']]\\\n",
    "                        .rename(columns = {'user_watch_count': 'user_total_watch_count_last_3_days'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge item features\n",
    "main_df = pd.merge(\n",
    "    interactions_filtered, movies_metadata[['movieId']+ ITEM_FEATURES_LIST],\n",
    "    how = 'left', on = 'movieId'\n",
    "                   ).drop_duplicates().reset_index(drop = True)\n",
    "assert main_df.shape[0] == interactions_filtered.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = main_df.sort_values('dttm').reset_index(drop = True)\n",
    "daily_users_feature = daily_users_feature.sort_values('dttm').reset_index(drop = True)\n",
    "cumulative_total_cnt = cumulative_total_cnt.sort_values('dttm').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge user features with watch count\n",
    "main_df = pd.merge_asof(\n",
    "    main_df, daily_users_feature,\n",
    "    on = 'dttm', by = 'userId',\n",
    "    direction = 'backward',\n",
    "    allow_exact_matches = True\n",
    "    )\n",
    "assert main_df.shape[0] == interactions_filtered.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.merge_asof(\n",
    "    main_df, cumulative_total_cnt,\n",
    "    on = 'dttm', by = 'userId',\n",
    "    direction = 'backward',\n",
    "    allow_exact_matches = True\n",
    "    )\n",
    "assert main_df.shape[0] == interactions_filtered.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_total_cnt.loc[cumulative_total_cnt['userId'] == 671]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp  = main_df.loc[main_df['userId'] == 671][['userId', 'dttm']]\n",
    "# pd.merge_asof(\n",
    "#     tmp.sort_values('dttm'), cumulative_total_cnt.sort_values('dttm'),\n",
    "#     on = 'dttm', by = 'userId', direction = 'backward',\n",
    "#     allow_exact_matches = True).sort_values('dttm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anyway we left some NaN\n",
    "main_df.isnull().sum() / len(main_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_FEATURES_LIST = ['revenue', 'budget', 'runtime', 'user_mean_rating',\n",
    "                       'user_watch_count', 'user_total_watch_count_last_3_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_COLS = ['userId', 'movieId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = main_df[ID_COLS + FINAL_FEATURES_LIST]\n",
    "y = main_df[TARGET]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size = TEST_SIZE,\n",
    "    random_state = RANDOM_STATE)\n",
    "\n",
    "print(f'Shape of train set X, y: {X_train.shape}, {len(y_train)}')\n",
    "print(f'Shape of train set X, y: {X_test.shape}, {len(y_test)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "model = CatBoostRegressor(\n",
    "    loss_function = 'MAE',\n",
    "    iterations = 2000,\n",
    "    learning_rate = .1,\n",
    "    depth = 6,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_test, y_test),\n",
    "    early_stopping_rounds = 20 # to avoid overfitting,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rekko-handbook-FR-mDNY1-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2d007ce7c74fe41dcd6cb53c80a05eb04c24d9c4918ecd21236b3a1c808c520"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
