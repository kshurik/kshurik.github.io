{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c28979",
   "metadata": {},
   "source": [
    "(chapter1_part3)=\n",
    "\n",
    "# Content-based and Collaborative Filterings in a Nutshell\n",
    "In this section, we will go through 2 straightforward ways to generate candidates for recommendations.\n",
    "As we mentioned before, these are *content-based* and *collaborative* filterings. Yet,\n",
    "we will go through explanation of both methods with examples and finnally discuss various libraries\n",
    "to implement them. Before that we have to define and understand embeddings. As you might have noticed,\n",
    "we mentioned a lot \"similar items\", \"similar users\" etc and question arises -- how we define that similarity?\n",
    "Speaking of calculation of similarity it is pretty straightforward -- we calcualte cosine between two arrays.\n",
    "The intruging part is how do we get these arrays from our data.\n",
    "\n",
    "## Embeddings Explained\n",
    "The evolution of text processing started from one-hot encoding. When there was text data, Data Scientists\n",
    "would preprocess them (lower case, remove symbols etc.) and then create one-hot representations of words or\n",
    "n-grams (when we split words/text into 2-3-...-n parts by characters). Finally, use some ML model on top of it.\n",
    "Notwithstanding the fact of easiness and interpretability of this approach, human language is sophisticated\n",
    "and various words can mean different meanings depending on the context and such techinique fails in most cases.\n",
    "\n",
    "Therefore, embeddings have become a next stage in text processing pipeline. It is type of word representation\n",
    "that allows words with similar meaning to have a similar representation. Unlike methods such as one-hot encoding,\n",
    "word embeddings provide a way to represent words in a more meaningful way, by mapping them to a vector of real\n",
    "numbers in a continuous vector space. The idea behind word embedding is to use a neural network to learn\n",
    "relationships between words in a dataset. The neural network is trained to assign a numeric vector to each word\n",
    "in the dataset. The vector is typically of fixed length and the goal is to find a vector that accurately\n",
    "represents the meaning of the word, in the context of the dataset. This allows for words in similar contexts\n",
    "to have similar vector representations. \n",
    "\n",
    "For example, imagine a dataset of movie reviews. Let’s say that the neural network has been trained to assign\n",
    "a vector to each word in the dataset. If the word “amazing” is used in a movie review, then the vector assigned\n",
    "to “amazing” will be similar to the vector assigned to “incredible”. This is because the meanings of these two\n",
    "words are similar and they are often used in similar contexts. Word embeddings can also be used to identify\n",
    "relationships between words. For example, consider the words “man” and “woman”. If the neural network assigned\n",
    "similar vectors to these two words, this would indicate that the two words are related.  In addition to\n",
    "identifying relationships between words, word embeddings can also be used to classify documents. For example,\n",
    "if a document contains the words “amazing” and “incredible”, then the neural network can assign an appropriate\n",
    "vector to each of these words. If a second document contains similar words, then the neural network can assign\n",
    "similar vectors to these words. This allows the neural network to accurately classify the documents as being similar. \n",
    "\n",
    "Finally, word embeddings can be used for data visualization. By plotting the vectors assigned to words in\n",
    "a two-dimensional space, it is possible to see how words are related. This can be a useful tool for understanding\n",
    "the relationships between words in a given dataset. In summary, word embeddings are a powerful tool\n",
    "for representing words in a meaningful way. They can be used to identify relationships between words,\n",
    "sclassify documents, and visualize data. \n",
    "\n",
    "Now, let's consider *content-based filttering* and use simple Word2Vec/Doc2Vec\n",
    "model to get such recommendations.\n",
    "\n",
    "## Content-based Filtering\n",
    "Content-based filtering can be used in a variety of applications, from recommending films and music to suggesting\n",
    "restaurants and travel destinations. In this part, we'll discuss how content-based filtering works and provide\n",
    "some examples.\n",
    "\n",
    "Content-based filtering is a type of recommender system that recommends items to users based on their past\n",
    "preferences and behaviors. It works by analyzing a user's preferences, in terms of attributes such as genre,\n",
    "director, actor, or even a combination of these, and then recommending other items that have similar attributes.\n",
    "For example, if a user has previously watched romantic comedies with Julia Roberts, content-based filtering\n",
    "would recommend other romantic comedies with Julia Roberts, or other films featuring similar actors or directors.\n",
    "\n",
    "Content-based filtering is based on the assumption that users who liked one item will likely like similar items.\n",
    "To generate recommendations, the system first identifies the attributes of the items that the user has previously\n",
    "interacted with. It then identifies other items that have similar attributes and recommends them to the user.\n",
    "For example, if a user has previously listened to Taylor Swift songs, the system will identify other Taylor Swift\n",
    "songs as well as songs with similar attributes, such as a similar genre or artist. In industry, this type of\n",
    "recommendations is showed with \"Similar to ...\". It is additional nudge to increase interest of a user\n",
    "as recommendations with explanation seems to be really personalized from the user's point of view.\n",
    "\n",
    "In conclusion, content-based filtering is a type of recommender system that recommends items to users based on their\n",
    "past preferences and behaviors. Next, we jump to coding part and create simple Word2Vec model via [`gensim`](https://pypi.org/project/gensim/) library.\n",
    "Well explained logic of Word2Vec model you can find [here](https://israelg99.github.io/2017-03-23-Word2Vec-Explained/).\n",
    "Here, we will not discuss details of implementation.\n",
    "\n",
    "### gensim: example of content-based recommendations based on Doc2Vec approach\n",
    "Now, we move on to implementation of content-based recommender using `gensim` library and Doc2Vec. It is almost\n",
    "the same as Word2Vec with sligh modification, but idea remains the same.\n",
    "\n",
    "#### 0. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5da8ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# links to shared data MovieLens\n",
    "# source on kaggle: https://www.kaggle.com/code/quangnhatbui/movie-recommender/data\n",
    "MOVIES_METADATA_URL = 'https://drive.google.com/file/d/19g6-apYbZb5D-wRj4L7aYKhxS-fDM4Fb/view?usp=share_link'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e3123c",
   "metadata": {},
   "source": [
    "#### 1. Modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "154802f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/s.khalilbekov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to make it available to download w/o SSL verification\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from ast import literal_eval\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# download stop words beforehand\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93bed3b",
   "metadata": {},
   "source": [
    "##### 1.1. Helper functions to avoid copypaste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c6f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_gdrive(url):\n",
    "    \"\"\"\n",
    "    gets csv data from a given url (taken from file -> share -> copy link)\n",
    "    :url: example https://drive.google.com/file/d/1BlZfCLLs5A13tbNSJZ1GPkHLWQOnPlE4/view?usp=share_link\n",
    "    \"\"\"\n",
    "    file_id = url.split('/')[-2]\n",
    "    file_path = 'https://drive.google.com/uc?export=download&id=' + file_id\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32300467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init lemmatizer to avoid slow performance\n",
    "mystem = Mystem() \n",
    "\n",
    "def word_tokenize_clean(doc: str, stop_words: list):\n",
    "    '''\n",
    "    tokenize from string to list of words\n",
    "    '''\n",
    "\n",
    "    # split into lower case word tokens \\w lemmatization\n",
    "    tokens = list(set(mystem.lemmatize(doc.lower())))\n",
    "  \n",
    "    # remove tokens that are not alphabetic (including punctuation) and not a stop word\n",
    "    tokens = [word for word in tokens if word.isalpha() and not word in stop_words \\\n",
    "              not in list(punctuation)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d1654",
   "metadata": {},
   "source": [
    "#### 2. Main\n",
    "##### 2.1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b638017c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adult                     object\n",
       "belongs_to_collection     object\n",
       "budget                    object\n",
       "genres                    object\n",
       "homepage                  object\n",
       "id                        object\n",
       "imdb_id                   object\n",
       "original_language         object\n",
       "original_title            object\n",
       "overview                  object\n",
       "popularity                object\n",
       "poster_path               object\n",
       "production_companies      object\n",
       "production_countries      object\n",
       "release_date              object\n",
       "revenue                  float64\n",
       "runtime                  float64\n",
       "spoken_languages          object\n",
       "status                    object\n",
       "tagline                   object\n",
       "title                     object\n",
       "video                     object\n",
       "vote_average             float64\n",
       "vote_count               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv information about films etc\n",
    "movies_metadata = read_csv_from_gdrive(MOVIES_METADATA_URL)\n",
    "movies_metadata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf1521",
   "metadata": {},
   "source": [
    "To get accurate results we need to preprocess text a bit. The pipeline will be as follows:\n",
    "- Filter only necessary columns from movies_metadada : id, original_title, overview;\n",
    "- Define `model_index` for model to match back with `id` column;\n",
    "- Text cleaning: removing stopwords & punctuation, lemmatization for further tokenization and tagged document creatin required for gensim.Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d504739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45466 entries, 0 to 45465\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   id              45466 non-null  object\n",
      " 1   original_title  45466 non-null  object\n",
      " 2   overview        44512 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# filter cols\n",
    "sample = movies_metadata[['id', 'original_title', 'overview']].copy()\n",
    "sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03015183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                0\n",
       "original_title    0\n",
       "overview          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you see from above, we have missing overview in some cases -- let's fill it with the original title\n",
    "sample.loc[sample['overview'].isnull(), 'overview'] = sample.loc[sample['overview'].isnull(), 'original_title']\n",
    "sample.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d871c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model_index and make it as string\n",
    "sample = sample.reset_index().rename(columns = {'index': 'model_index'})\n",
    "sample['model_index'] = sample['model_index'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "224c3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapper with title and model_idnex to use it further in evaluation\n",
    "movies_inv_mapper = dict(zip(sample['original_title'].str.lower(), sample['model_index'].astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76d4dc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Led by Woody, Andy's toys live happily in his room until Andy's birthday brings Buzz Lightyear onto the scene. Afraid of losing his place in Andy's heart, Woody plots against Buzz. But when circumstances separate Buzz and Woody from their owner, the duo eventually learns to put aside their differences.\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess by removing non-character data, stopwords\n",
    "tags_corpus = sample['overview'].values\n",
    "tags_corpus = [re.sub('-[!/()0-9]', '', x) for x in tags_corpus]\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "tags_doc = [word_tokenize_clean(description, stop_words) for description in tags_corpus]\n",
    "tags_corpus[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de5fc3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data as model input for Word2Vec\n",
    "## it takes some time to execute\n",
    "tags_doc = [TaggedDocument(words = word_tokenize_clean(D, stop_words), tags = [str(i)]) for i, D in enumerate(tags_corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5eb5fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['board', 'magical', 'door', 'alan', 'siblings', 'unwittingly', 'discover', 'invite', 'running', 'proves', 'evil', 'judy', 'world', 'creatures', 'living', 'finish', 'years', 'three', 'monkeys', 'game', 'freedom', 'risky', 'terrifying', 'inside', 'enchanted', 'giant', 'find', 'adult', 'peter', 'opens', 'trapped', 'hope', 'rhinoceroses', 'room'], tags=['1'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check what do we have\n",
    "## tag = movie index\n",
    "tags_doc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda30e6c",
   "metadata": {},
   "source": [
    "#### 2.2. Model Training and Evaluation\n",
    "\n",
    "First, let's define some paramters for Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5d33c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VEC_SIZE = 50 # length of the vector for each movie\n",
    "ALPHA = .02 # model learning param\n",
    "MIN_ALPHA = .00025 # model learning param\n",
    "MIN_COUNT = 5 # min occurrence of a word in dictionary\n",
    "EPOCHS = 20 # number of trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbb73b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = Doc2Vec(vector_size = VEC_SIZE,\n",
    "                alpha = ALPHA, \n",
    "                min_alpha = MIN_ALPHA,\n",
    "                min_count = MIN_COUNT,\n",
    "                dm = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9f2302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate vocab from all tag docs\n",
    "model.build_vocab(tags_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5102777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model.train(tags_doc,\n",
    "            total_examples = model.corpus_count,\n",
    "            epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba53e1b3",
   "metadata": {},
   "source": [
    "Now, let's make some checks by defining parameters for model ourselves.\n",
    "Assume that we watched movie `batman` and based on that generate recommendation similar to it's description.\n",
    "To do that we need:\n",
    "- To extract movie id from `movies_inv_mapper` we created to map back titles from model output\n",
    "- Load embeddings from trained model\n",
    "- Use built-in most_similar() method to get most relevant recommendations based on film embedding\n",
    "- Finally, map title names for sense-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8120efe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8603"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get id\n",
    "movie_id = movies_inv_mapper['batman']\n",
    "movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da39b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained embeddings \n",
    "movies_vectors = model.dv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd1522dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_embeddings = movies_vectors[movie_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adb4fdbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_index</th>\n",
       "      <th>model_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8603</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12330</td>\n",
       "      <td>0.961295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7772</td>\n",
       "      <td>0.953292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5713</td>\n",
       "      <td>0.953070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43165</td>\n",
       "      <td>0.951156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_index  model_score\n",
       "0        8603     1.000000\n",
       "1       12330     0.961295\n",
       "2        7772     0.953292\n",
       "3        5713     0.953070\n",
       "4       43165     0.951156"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get recommendations\n",
    "similars = model.docvecs.most_similar(positive = [movie_embeddings], topn = 20)\n",
    "output = pd.DataFrame(similars, columns = ['model_index', 'model_score'])\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fffc3aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse values and indices to map names in dataframe\n",
    "name_mapper = {v: k for k, v in movies_inv_mapper.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e41b924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_index</th>\n",
       "      <th>model_score</th>\n",
       "      <th>title_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8603</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>batman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12330</td>\n",
       "      <td>0.961295</td>\n",
       "      <td>the detonator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7772</td>\n",
       "      <td>0.953292</td>\n",
       "      <td>this island earth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5713</td>\n",
       "      <td>0.953070</td>\n",
       "      <td>rollover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43165</td>\n",
       "      <td>0.951156</td>\n",
       "      <td>the zookeeper's wife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29872</td>\n",
       "      <td>0.951120</td>\n",
       "      <td>angels die hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13835</td>\n",
       "      <td>0.949979</td>\n",
       "      <td>k2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>43461</td>\n",
       "      <td>0.948913</td>\n",
       "      <td>megafault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22454</td>\n",
       "      <td>0.946273</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>28001</td>\n",
       "      <td>0.945889</td>\n",
       "      <td>reach me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>26511</td>\n",
       "      <td>0.945728</td>\n",
       "      <td>the 7 adventures of sinbad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>43019</td>\n",
       "      <td>0.945695</td>\n",
       "      <td>santo en anónimo mortal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>30134</td>\n",
       "      <td>0.945451</td>\n",
       "      <td>spy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>23198</td>\n",
       "      <td>0.945309</td>\n",
       "      <td>calvary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>19426</td>\n",
       "      <td>0.944578</td>\n",
       "      <td>suicide club</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>44339</td>\n",
       "      <td>0.944103</td>\n",
       "      <td>the underground world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>42040</td>\n",
       "      <td>0.944033</td>\n",
       "      <td>equalizer 2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16100</td>\n",
       "      <td>0.943540</td>\n",
       "      <td>tales of an ancient empire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19869</td>\n",
       "      <td>0.943080</td>\n",
       "      <td>the lost missile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19227</td>\n",
       "      <td>0.942782</td>\n",
       "      <td>carbon nation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_index  model_score                  title_name\n",
       "0         8603     1.000000                      batman\n",
       "1        12330     0.961295               the detonator\n",
       "2         7772     0.953292           this island earth\n",
       "3         5713     0.953070                    rollover\n",
       "4        43165     0.951156        the zookeeper's wife\n",
       "5        29872     0.951120             angels die hard\n",
       "6        13835     0.949979                          k2\n",
       "7        43461     0.948913                   megafault\n",
       "8        22454     0.946273                         NaN\n",
       "9        28001     0.945889                    reach me\n",
       "10       26511     0.945728  the 7 adventures of sinbad\n",
       "11       43019     0.945695     santo en anónimo mortal\n",
       "12       30134     0.945451                         spy\n",
       "13       23198     0.945309                     calvary\n",
       "14       19426     0.944578                suicide club\n",
       "15       44339     0.944103       the underground world\n",
       "16       42040     0.944033              equalizer 2000\n",
       "17       16100     0.943540  tales of an ancient empire\n",
       "18       19869     0.943080            the lost missile\n",
       "19       19227     0.942782               carbon nation"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['title_name'] = output['model_index'].astype(int).map(name_mapper)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e33230",
   "metadata": {},
   "source": [
    "## Collaborative Filtering\n",
    "Collaborative filtering is a powerful method for recommendation systems used to predict user preferences or\n",
    "interests. It is based on the notion that people who have similar tastes and preferences in one domain are likely\n",
    "to have similar tastes and preferences in a different domain. The collaborative filtering technique seeks to identify\n",
    "users who have similar tastes and preferences, based on their past interactions, and then use those users'\n",
    "interactions of items to predict relevance of similar items for the user. The goal of collaborative filtering is\n",
    "to use the opinions of other people to make predictions about a user’s preferences and interests.\n",
    "This is done by finding users who have similar tastes and preferences as the user in question, and then using\n",
    "those users’ ratings of items to make predictions about how the user would rate the same items.\n",
    "There are two main approaches to collaborative filtering: memory-based and model-based. \n",
    "\n",
    "### Memory-based Collaborative Filtering\n",
    "Memory-based collaborative filtering, also known as neighborhood-based collaborative filtering, is an approach\n",
    "that relies on finding similar users or items based on their behavior or preferences. The basic idea is to use\n",
    "the ratings or interactions of users with items to identify other users who have similar tastes, and then use\n",
    "the ratings of those similar users to make recommendations to a target user. One common approach in memory-based\n",
    "collaborative filtering is user-based collaborative filtering. In this approach, the similarity between users is\n",
    "calculated based on their ratings for items. A similarity metric such as the cosine similarity or Pearson correlation\n",
    "coefficient is often used to measure the similarity between two users. The similarity scores are then used to\n",
    "identify the most similar users to the target user. Once the most similar users are identified, their ratings\n",
    "for items are used to generate recommendations for the target user. Item-based collaborative filtering is another\n",
    "common approach in memory-based collaborative filtering. In this approach, the similarity between items is calculated\n",
    "based on the ratings of users who have rated both items. The similarity scores are then used to identify items that\n",
    "are similar to the items that the target user has already rated highly. Once the similar items are identified,\n",
    "they are recommended to the target user. One advantage of memory-based collaborative filtering is that it is easy\n",
    "to implement and interpret. The algorithm is relatively simple and does not require a lot of computational resources.\n",
    "Additionally, memory-based collaborative filtering can be effective when there is a lot of data available and the\n",
    "user-item matrix is sparse. However, memory-based collaborative filtering also has several disadvantages.\n",
    "One major limitation is that it is prone to the cold-start problem, which occurs when there is not enough data\n",
    "available to identify similar users or items. Additionally, memory-based collaborative filtering can be\n",
    "computationally expensive when there are a large number of users or items.\n",
    "\n",
    "Let's consider an example with Pearson Correlation\n",
    "Say, we have a dataset that contains the ratings of four users on five movies. The data looks like this:\n",
    "\n",
    "|         | User A | User B | User C | User D |\n",
    "|-------- | -------- | ------- | ------- | --------     |\n",
    "|Movie 1 | 5 | 4 | 2 | 3 | \n",
    "|Movie 2 | 3 | 3 | 4 | 4 | \n",
    "|Movie 3 | 4 | 4 | 5 | 5 |\n",
    "|Movie 4 | 1 | 2 | 1 | 2 |\n",
    "|Movie 5 | 2 | 1 | 3 | 3 |\n",
    "\n",
    "\n",
    "To apply collaborative filtering, we can compute the similarity between each pair of users based on their \n",
    "ratings. The similarity is calculated using the Pearson Correlation Coefficient (PCC). The PCC is a measure\n",
    "of how well two sets of data are correlated, and it ranges from -1 (perfectly negatively correlated) to +1\n",
    "(perfectly positively correlated). For example, let’s assume that we want to find the similarity between\n",
    "User A and User B. The PCC is calculated by taking the average of the product of the ratings for each movie.\n",
    "So, let's get PCC for User A and User B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65ab43bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation for user A and B is: 0.8488746876271654\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "user_a = [5, 3, 4, 1, 2]\n",
    "user_b = [4, 3, 4, 2, 1]\n",
    "\n",
    "print(f'Pearson Correlation for user A and B is: {np.corrcoef(user_a, user_b)[0, 1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b6dee",
   "metadata": {},
   "source": [
    "It indicates that `User A` and `User B` have a strong positive correlation in their ratings.\n",
    "To find recommendations for `User A`, we can first identify the users who are most similar to `User A`.\n",
    "In this example, that would be `User B` and `User C` (check it out by calculating other pairs).\n",
    "Next, we can take the weighted average of the ratings from those users for the movies that `User A`\n",
    "has not yet rated. For example, let’s assume that `User A` has not yet rated Movie 4. We can then take\n",
    "the weighted average of the ratings for Movie 4 from `User B` and `User C`.  `User B` rated Movie 4 a 2,\n",
    "and `User C` rated it a 1. We can then take the weighted average of those ratings, giving more weight \n",
    "to `User B` since they are more similar to `User A`. In this case, the weighted average would be around 2.\n",
    "Therefore, based on the ratings from other users, it is likely that `User A` would rate Movie 4 a 2.\n",
    "\n",
    "To wrap up, we can say that memory-based collaborative filtering as about calculating similarity\n",
    "between rows or columns of interaction matrix. In our example, we took columns a.k.a user similarities\n",
    "while we could take item-item similarities and use as recommendation.\n",
    "\n",
    "\n",
    "### Model-based Collaborative Filtering\n",
    "Model-based collaborative filtering is an approach that uses machine learning algorithms to learn a model from \n",
    "the ratings or interactions of users with items. The model is then used to make predictions about the relevance of\n",
    "users for items that they have not yet interacted with. One common approach in model-based collaborative filtering\n",
    "is matrix factorization. In this approach, the user-item matrix is decomposed into two lower-dimensional matrices:\n",
    "a user matrix and an item matrix. The user matrix represents the latent preferences of users, and the item matrix\n",
    "represents the latent attributes of items. The dot product of the user and item matrices gives the predicted relevance\n",
    "for a user-item pair. Matrix factorization is typically performed using a technique called Singular Value Decomposition (SVD).\n",
    "The example of how it is computed is showed below. Basically, we have interactions data where rows represent\n",
    "users and columns their ratings/other interactions. Based on thatm we have find such matrices that would approximate\n",
    "this relationship from our interactions data.\n",
    "\n",
    "![](img/svd_example.png)\n",
    "*Toy example with SVD decomposition*\n",
    "\n",
    "However, SVD is computationally expensive and may not scale well to large datasets. Therefore, alternative techniques\n",
    "such as Alternating Least Squares (ALS) or modification for implicit target iALS, Stochastic Gradient Descent (SGD)\n",
    "are often used. Another common approach in model-based collaborative filtering is deep learning.\n",
    "In this approach, a neural network is used to learn a representation of users and items.\n",
    "The network takes as input the ratings or interactions of users with items and outputs a prediction of the rating for a\n",
    "user-item pair. Deep learning has the advantage of being able to capture complex patterns in the data and can be used\n",
    "to learn non-linear relationships between users and items. One of the popular examples is Extreeme Deep Factorization machines (xDeepFM).\n",
    "One advantage of model-based collaborative filtering is that it can handle the cold-start problem by using the\n",
    "learned model to make predictions about items that have not yet been rated by users. Additionally, model-based\n",
    "collaborative filtering can be more accurate than memory-based collaborative filtering, especially when there are\n",
    "a large number of users and items. Obviously, if we have enough data we can generate more accurate predictions minimizing our loss function\n",
    "However, model-based collaborative filtering also has some disadvantages. One major limitation is that it can be\n",
    "difficult to interpret the learned model and understand why certain recommendations are being made. Additionally,\n",
    "model-based collaborative filtering can be computationally expensive and may require a lot  of computational resources,\n",
    "especially when using deep learning techniques. Another disadvantage of model-based collaborative filtering is that it\n",
    "requires a large amount of data to train the model effectively. This can be a challenge in some domains, where there\n",
    "may be a limited amount of data available. In these cases, memory-based collaborative filtering may be a better choice.\n",
    "\n",
    "**TODO ADD PYTHON CODE FOR COLLABORATIVE FILTERING HERE HERE**\n",
    "\n",
    "### Hybrid Approaches\n",
    "In practice, many recommender systems use a hybrid approach that combines both memory-based and model-based\n",
    "collaborative filtering. In a hybrid approach, the strengths of both approaches are leveraged to improve the\n",
    "accuracy and performance of the recommender system. One common approach in hybrid collaborative filtering is\n",
    "to use a memory-based approach to generate initial recommendations and then refine the recommendations using\n",
    "a model-based approach. This approach can be effective in situations where there is not enough data to train\n",
    "a model effectively but there is enough data to identify similar users or items using a memory-based approach.\n",
    "Another approach is to use a model-based approach to generate initial recommendations and then refine the\n",
    "recommendations using a memory-based approach. This approach can be effective in situations where the user-item\n",
    "matrix is very sparse and a model-based approach is needed to make accurate predictions.\n",
    "\n",
    "\n",
    "## Python Libraries for Implementations\n",
    "There are many Python libraries available for content-based filtering such as Surprise & LightFM.\n",
    "One of the most popular libraries is Surprise, which is a Python machine learning library for\n",
    "recommendation systems. It includes several algorithms for making predictions and performing content-based filtering.\n",
    "\n",
    "Another library is LightFM, which is a Python library for building recommendation systems. It includes a set of\n",
    "algorithms for content-based filtering, such as the weighting of item attributes and personalized rankings.\n",
    "\n",
    "\n",
    "## TODO\n",
    "- discuss more deeply python libraries implementation for collaborative filtering in terms features / time"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "source_map": [
   11,
   89,
   93,
   96,
   118,
   120,
   133,
   149,
   154,
   158,
   165,
   170,
   176,
   182,
   187,
   197,
   203,
   207,
   212,
   220,
   229,
   234,
   239,
   249,
   255,
   260,
   264,
   271,
   276,
   279,
   332,
   339
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}